data(mtcars)
lm(mtcars$mpg ~ mtcars$weight)
summary(mtcars$weight)
colnames(mtcars)
lm(mtcars$mpg ~ mtcars$wt)
x <- c(8.58, 10.46, 9.01, 9.64, 8.86)
x - mean(x) / sd(x)
x <- c(0.8, 0.47, 0.51, 0.73, 0.36, 0.58, 0.57, 0.85, 0.44, 0.42)
y <- c(1.39, 0.72, 1.55, 0.48, 1.19, -1.59, 1.23, -0.65, 1.49, 0.05)
lm(y~x)
x <- c(0.8, 0.47, 0.51, 0.73, 0.36, 0.58, 0.57, 0.85, 0.44, 0.42)
x <- c(0.8, 0.47, 0.51, 0.73, 0.36, 0.58, 0.57, 0.85, 0.44, 0.42)
lm(x)
y <- 0.573
sum(x-y)^2
y <- 0.8
sum(x-y)^2
y <- 0.36
sum(x-y)^2
y <- 0.44
sum(x-y)^2
(0.8-0.36)^2+(0.47-0.36)^2+(0.51-0.36)^2+(0.73-0.36)^2+(0.36-0.36)^2+(0.58-0.36)^2+(0.57-0.36)^2+(0.85-0.36)^2+(0.44-0.36)^2+(0.42-0.36)^2
sum((x-y)^2
)
y <- 0.36
sum((x-y)^2)
y <- 0.573
sum((x-y)^2)
y <- 0.8
sum((x-y)^2)
y <- 0.36
sum((x-y)^2)
y <- 0.44
sum((x-y)^2)
x <- c(0.18, -1.54, 0.42, 0.95)
w <- c(2, 1, 3, 1)
mu <- 0.1471
sum(w * ( x - mu) ^ 2)
mu <- 0.0025
sum(w * ( x - mu) ^ 2)
mu <- 0.3
sum(w * ( x - mu) ^ 2)
mu <- 1.077
sum(w * ( x - mu) ^ 2)
x <- c(8.58, 10.46, 9.01, 9.64, 8.86)
(x - mean(x)) / sd(x)
x <- c(0.8, 0.47, 0.51, 0.73, 0.36, 0.58, 0.57, 0.85, 0.44, 0.42)
y <- c(1.39, 0.72, 1.55, 0.48, 1.19, -1.59, 1.23, -0.65, 1.49, 0.05)
lm(y~x)
x <- (x-mean(x)/sd(x))
x <- c(0.8, 0.47, 0.51, 0.73, 0.36, 0.58, 0.57, 0.85, 0.44, 0.42)
x <- ((x-mean(x))/sd(x))
x
y <- ((y - mean(y))/sd(y))
lm( y ~ x)
x <- c(0.8, 0.47, 0.51, 0.73, 0.36, 0.58, 0.57, 0.85, 0.44, 0.42)
y <- c(1.39, 0.72, 1.55, 0.48, 1.19, -1.59, 1.23, -0.65, 1.49, 0.05)
lm (x ~ y)
x <- c(0.8, 0.47, 0.51, 0.73, 0.36, 0.58, 0.57, 0.85, 0.44, 0.42)
y <- c(1.39, 0.72, 1.55, 0.48, 1.19, -1.59, 1.23, -0.65, 1.49, 0.05)
lm(y ~ -1+x)
nosim <- 1000
n <- 10
sd(apply(matrix(rnorm(nosim * n), nosim), 1, mean))
1/sqrt(n)
sd(apply(matrix(runif(nosim * n), nosim), 1, mean))
1/sqrt(12*n)
sd(apply(matrix(rpois(nosim * n,4), nosim), 1, mean))
2/sqrt(n)
sd(apply(matrix(sample(0:1, nosim * n,replace = T), nosim), 1, mean))
1/(2*sqrt(n))
library(UsingR)
data(father.son)
x <- father.son$sheight
n <- length(x)
n
hist(x)
?hist
hist(x, breaks = 12)
hist(x, breaks = 20)
hist(x, breaks = 30)
hist(x, probablity = !freq)
hist(x, probablity = T)
hist(x, probablity = F)
round(c(var(x), var(x)/n, sd(x), sd(x)/sqrt(n)),2)
0.56 + c(-1, 1) * qnorm(0.975) * sqrt((0.56*0.44)/100)
0.56 + c(-1, 1) * qnorm(0.975) * sqrt(0.56*0.44/100)
poisson.test(5, T = 94.32)$conf.int
poisson.test(5, T = 94.32)$conf
pnorm(70, mean = 80, sd = 10)
qnorm(0.95, mean = 1100, sd = 75)
qnorm(0.95, mean = 1100, sd = 75^2/100)
qnorm(0.95, mean = 1100, sd = 75/100)
pbinom(4:5,size = 5, prob=0.5, lower.tail=F)
pbinom(4,size = 5, prob=0.5, lower.tail=F)+pbinom(5,size = 5, prob=0.5, lower.tail=F)
pnorm(16, mean =15, sd=10)
pnorm(16, mean =15, sd=10) - pnorm(14, mean = 15, sd = 10)
pnorm(14, mean = 15, sd = 10)
pnorm(16, mean =15, sd=10^2/100) - pnorm(14, mean = 15, sd = 10^2/100)
?punif
ppois(10, lambda = 5 * 3)
qnorm(0.95, mean = 1100, sd = 75^2/100)
qnorm(0.95, mean = 1100, sd = 75)
qnorm(0.95, mean = 1100, sd = 75^2/100)
round(pnorm(.5, mean = 0.5, sd = sqrt(1 / 12 / 1000), lower.tail = FALSE), 3)
?qnorm
dnorm(0.95, mean = 1100, sd = 75^2/100)
qnorm(0.95, mean = 1100, sd = 75^2/100)
pnorm(0.95, mean = 1100, sd = 75^2/100)
pbinom(5, size=5, prob=0.5)
pnorm(0.95, mean = 1100, sd = sqrt(75^2/100)
)
pnorm(0.95, mean = 1100, sd = sqrt(75^2/100))
sqrt(75^2/100)
pnorm(0.95, mean = 1100, sd = 7.5)
pnorm(0.95, mean = 1100, sd = 10)
qnorm(0.95, mean=1100, sd = 7.5)
pbinom(4, size =5, prob=0.5, lower.tail=F)
pbinom(5, size =5, prob=0.5, lower.tail=F)
round(pbinom(4, prob = .5, size = 6, lower.tail = FALSE) * 100, 1)
pbinom(4, prob = .5, size = 6, lower.tail = FALSE)
pbinom(4, prob = .5, size = 5, lower.tail = FALSE)
pbinom(5, prob = .5, size = 5, lower.tail = FALSE)
0.5^5
4*0.5^4
0.5^4
LIBRARY(SWIRL)
library(swirl)
install_from_swirl("Statistical Inference")
swirl()
33/36
deck
1/52
52
4/52
0
12/52
2/51
pbinom(3, size =5, prob=0.5, lower.tail=F)
library(UsingR)
data(diamond)
plot(diamond$caract, diamond$price, )
plot(diamond$caract, diamond$price )
plot(diamond$carat, diamond$price )
abline(lm(proce ~ carat, data = diamond), lwd=2)
abline(lm(price ~ carat, data = diamond), lwd=2)
fit <- lm(price ~ carat, data = diamond)
coef(fit)
fit2 <- lm(price ~ I(carat - mean(carat)), data=diamond)
coef(fit2)
fit3 <- lm(price ~ I(carat * 10), data = diamond)
coef(fit3)
newx <- c(0.16, 0.27, 0.34)
coef(fit)[1] + coef(fit)[2] * newx
predict(fit, newdata = data.frame(carat = newx))
data(anscombe)
example(anscombe)
x <- c(0.61, 0.93, 0.83, 0.35, 0.54, 0.16, 0.91, 0.62, 0.62)
y <- c(0.67, 0.84, 0.6, 0.18, 0.85, 0.47, 1.1, 0.65, 0.36)
lm(y ~ y)
lm(y ~ x)
?test
?t.test
t.test(y ~ x)
lm(y ~ x)
fit <- lm(y ~ x)
summary(fit)$coefficients
summary(fit)
mtcars
fit <- lm(mtcar$mpg ~ mtcars$wt)
fit <- lm(mtcars$mpg ~ mtcars$wt)
summary(fit)
plot(mtcars$mpg, mtcars$wt)
abline(fit)
abline(fit)
plot(mtcars$mpg, mtcars$wt)
abline(fit)
abline(v = mean(mtcars$mpg))
abline(h = mean(mtcars$wt))
abline(fit, lwd=2)
sumCoef <- summary(fit)$coefficients
sumCoef
sumCoef[2,1] - 1 * qt(0.975, df=fit$df) * sumCoef[2,2]
?mtcars
plot(mtcars$wt, mtcars$mpg)
abline(fit)
abline(v = mean(mtcars$wt))
abline(h = mean(mtcars$mpg))
abline(v = 3)
newx <- c(3)
coef(fit)[1]+coef(fit)[2] * newx
expY <- coef(fit)[1]+coef(fit)[2] * newx
expY + qt(0.975, df=fit$df) * sumCoef[1,2]
expY + qt(0.975, df=fit$df) * sumCoef[2,2]
coef(fit)[1,1]+coef(fit)[2,1] * newx
fit
expy <- coef(fit)[1]+coef(fit)[2] * newx
expy
expy + qt(0.975, df = sumCoef$df) * sumCoef[2,2]
expy + qt(0.975, df = fit$df) * sumCoef[2,2]
expy + qt(0.975, df = fit$df) * sumCoef[1,2]
fit <- lm(mtcars$mpg ~ I(2*mtcars$wt))
fit
sumCoef <- summary(fit)$coefficients
sumCoef
expy <- coef(fit)[1]+coef(fit)[2] * mean(y)
expy - qt(0.975, df = fit$df) * sumCoef[1,2]
fit <- (mtcars$mpg ~ mtcars$wt)
newdata = data.frame(wt = mean(mtcars$wt))
predict(fit, newdata, interval ="predict")
wtmean <- mean(mtcars$wt)
newdata = data.frame(wt = wtmean)
predict(fit, newdata, interval ="predict")
predict(fit, mean(mtcars$wt), interval ="predict")
?predict
fit
summary(fit)
predict(fit, mtcars$wt = mean(mtcars$wt), interval ="predict")
newdata = data.frame(mtcars$wt = wtmean)
newdata <- data.frame(mtcars$wt = wtmean)
summary(fit)$coef
lm(mtcars$mpg ~ mtcars$wt)
model <- lm(mtcars$mpg ~ mtcars$wt)
summary(model)
newdata <- data.frame("mtcars$wt" = mean(mtcars$wt))
predict(model, newdata, interval="predict")
mean(mtcars$wt)
model
predict(model)
predict(model, "mtcars$wt" = 3.21725, interval ="predict")
newdata = data.frame("mtcars$wt" = 3.21725, interval="predict")
newdata = data.frame("mtcars$wt" = 3.21725)
predict(model, newdata, interval = "predict")
model.lm <- lm(mtcars$mpg ~ mtcars$wt)
model.lm
summary(model.lm)
meanwt <- mean(metcars$wt)
meanwt <- mean(mtcars$wt)
meanwt
newdata = data.frame(mtcars$wt = meanwt)
newdata = data.frame("mtcars$wt" = meanwt)
newdata
model.lm <- lm(mtcars$mpg ~ "mtcarswt" = mtcars$wt)
model.lm <- lm(mtcars$mpg ~ "mtcars\$wt" = mtcars$wt)
model.lm <- lm(mtcars$mpg ~ "mtcars$wt" = mtcars$wt)
model.lm <- lm(mtcars$mpg ~ mtcars$wt)
summary(model.lm)
newdata
colnames(newdata) <- "mtcars$wt"
newdata
predict(model.lm, newdata, interval = "predict")
newdata
predict(model.lm, newdata, interval = "confidence")
predict(model.lm, newdata, interval="confidence")
attach(faithful)
eruption.lm = lm(eruptions ~ waiting)
summary(eruption.lm)
summary(faithful)
newdata = data.frame(waiting=80)
predict(eruption.lm, newdata, interval="confidence")
attach(mtcars)
model <- lm(mpg ~ wt)
newdata = data.frame(wt = mean(wt))
newdata
predict(model, newdata, interval="confidence")
newdata = data.frame(wt = 3)
newdata
predict(model, newdata, interval="prediction")
model <- lm(mpg ~ wt/2)
shorton <- mt/2
shorton <- wt/2
shorton
model <- lm(mpg ~ shorton)
predict(model, interval = "confidence")
model
cummary(mtcars)
summary(mtcars)
class(mtcars$am)
?mtcars
data <- mtcars
summary(data)
data$cyl <- as.factor(data$cyl)
fit <- lm(data$mpg ~ data$cyl + data$wt)
fit$coef
summary(dataÃ§)
summary(fit)
summary(lm(data$mpg ~ data$cyl))
lm(mpg ~ I(wt * 0.5) + factor(cyl), data = mtcars)
summary(lm(mpg ~ I(wt * 0.5) + factor(cyl), data = mtcars))
x <- c(0.586, 0.166, -0.042, -0.614, 11.72)
y <- c(0.549, -0.026, -0.127, -0.751, 1.344)
fit <- lm(y ~ x)
summary(fit)
plot(fit)
plot(x ~ y)
plot(y ~ x)
?diagnostics
?diagnostic
hatvalues(fit)
x <- c(0.586, 0.166, -0.042, -0.614, 11.72)
y <- c(0.549, -0.026, -0.127, -0.751, 1.344)
fit <- lm(y ~ x)
leverage <- hatvalues(fit)
leverage
leverage2 <- dfbeta(fit)
leverage2
leverage2 <- dfbeta(fit$5)
leverage2 <- dfbeta(fit[5])
?dfbeta
dfbetaPlots(fit)
dfbetasPlots(fit)
?dfbetaPlots
dfbeta(fit)
dfbetas(fit)
dfbetas(fit,5)
?dfbetas
data(mtcars)
fit1 <- lm(mpg~factor(cyl) + wt, data=mtcars)
fit2 <- update(fit1, mpg~factor(cyl) + wt + wt*factor(cyl))
summary(fit1)
summary(fit2)
data(sleep)
head(sleep)
g1 <- sleep$extra[1:10]
g2 <- sleep$extra[11:20]
difference <- g2-g1
mn <- mean(difference)
s <- sd(difference)
n <- 10
mn + c(-1,1) * qt(.975, n-1) * s / sqrt(n)
t.test(difference)
t.test(g2, g1, paired = T)
t.test(extra ~ I(relevel(group,2)), paired = T, data = sleep)
library(datasets)
dat(ChickWeight)
data(ChickWeight)
library(reshape2)
head(ChickWeight)
wideCW <- dcast(ChickWeight, Diet + Chick ~ Time, value.var = "weight")
head(wideCW)
names(wideCW)[-(1:2)] <- paste("time", names(wideCW)[-(1:2)], sep=" ")
head(wideCW)
library(dplyr)
wideCW <- mutate(wideCW, gain = time 21 - time 0)
wideCW <- mutate(wideCW, gain = "time 21" - "time 0")
names(wideCW)[-(1:2)] <- paste("time", names(wideCW)[-(1:2)], sep="")
head(wideCW)
?qt
library(ISLR)
install.packages("ISLR")
library(ISLR)
library(ggplot2)
library(caret)
data(Wage)
summary(Wage)
inTrain <- createDataPartition(y=Wage$wage, p=0.7, list = F)
training <- Wage[inTrain,]
testing <- Wage[-inTrain,]
dim(training);dim(testing)
featurePlot(x = training[], c("age", "education", "jobclass")], y=training$wage, plot="pairs")
featurePlot(x = training[, c("age", "education", "jobclass")], y=training$wage, plot="pairs")
library(AppliedPredictiveModeling)
library(caret)
data(AlzheimerDisease)
install.packages("AppliedPredictiveModeling")
data(AlzheimerDisease)
library(AppliedPredictiveModeling)
data(concrete)
library(caret)
set.seed(975)
inTrain = createDataPartition(mixtures$CompressiveStrength, p = 3/4)[[1]]
training = mixtures[ inTrain,]
testing = mixtures[-inTrain,]
summary(training)
qplot(ComprenssiveStrength, data=training)
qplot(CompressiveStrength, data=training)
dim(training)
qplot(CompressiveStrength, index, data=training)
dim(training)
splitOn <- cut2(training$Age, g = 4)
install.packages("Hmisc")
library(hmisc)
library(Hmisc)
splitOn <- cut2(training$Age, g = 4)
splitOn <- mapvalues(splitOn,
from = levels(factor(splitOn)),
to = c("red", "blue", "yellow", "green"))
plot(training$CompressiveStrength, col = splitOn)
summary(training)
splitOn <- cut2(training$Cement, g = 4)
plot(training$CompressiveStrength, col = splitOn)
splitOn <- cut2(training$BlastFurnaceSlag, g = 4)
plot(training$CompressiveStrength, col = splitOn)
library(AppliedPredictiveModeling)
data(concrete)
library(caret)
set.seed(975)
inTrain = createDataPartition(mixtures$CompressiveStrength, p = 3/4)[[1]]
training = mixtures[ inTrain,]
testing = mixtures[-inTrain,]
colnames(training)
hist(training$Superplasticizer)
summary(training$Superplasticizer)
library(caret)
library(AppliedPredictiveModeling)
set.seed(3433)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
colnames(training)
IL_variables <- grep("^IL", names(training), value = TRUE)
preProc <- preProcess(training[, IL_variables], method = "pca", thresh = 0.9)
preProc
library(caret)
library(AppliedPredictiveModeling)
set.seed(3433)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
## train the data using the first method
modelFit <- train(diagnosis ~ ., method = "glm", data = training)
predictions <- predict(modelFit, newdata = testing)
## get the confusion matrix for the first method
C1 <- confusionMatrix(predictions, testing$diagnosis)
print(C1)
A1 <- C1$overall[1]
## do similar steps with PCA
modelFit <- train(training$diagnosis ~ ., method = "glm", data = training,
preProcess = "pca",
Control = trainControl(preProcOptions = list(thresh = 0.8)))
C2 <- confusionMatrix(testing$diagnosis, predict(modelFit, testing))
print(C2)
library(caret)
library(AppliedPredictiveModeling)
set.seed(3433)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
set.seed(3433)
## grep the predictors starting with 'IL'
IL_str <- grep("^IL", colnames(training), value = TRUE)
## make a subset of these predictors
predictors_IL <- predictors[, IL_str]
# create a new DF of predictors and diagnosis
df <- data.frame(diagnosis, predictors_IL)
# create a training and testing set from this DF
inTrain = createDataPartition(df$diagnosis, p = 3/4)[[1]]
training = df[inTrain, ]
testing = df[-inTrain, ]
## train the data using the first method
modelFit <- train(diagnosis ~ ., method = "glm", data = training)
predictions <- predict(modelFit, newdata = testing)
## get the confusion matrix for the first method
C1 <- confusionMatrix(predictions, testing$diagnosis)
print(C1)
A1 <- C1$overall[1]
## do similar steps with PCA
modelFit <- train(training$diagnosis ~ ., method = "glm", data = training,
preProcess = "pca",
Control = trainControl(preProcOptions = list(thresh = 0.8)))
C2 <- confusionMatrix(testing$diagnosis, predict(modelFit, testing))
print(C2)
library(caret)
library(AppliedPredictiveModeling)
set.seed(3433)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
IL_variables <- grep("^IL", names(training), value = TRUE)
preProcPCA <- preProcess(training[, IL_variables], method = "pca", thresh = 0.8)
preProcNormal <- preProcess(training[, IL_variables], thresh = 0.8)
colnames(training)
modelFitPCA <- train(diagnosis ~ preProcPCA, data = training, method = "glm")
modelFitPCA <- train(diagnosis ~ ., data = preProcPCA, method = "glm")
library(caret)
library(AppliedPredictiveModeling)
set.seed(3433)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
set.seed(3433)
## grep the predictors starting with 'IL'
IL_str <- grep("^IL", colnames(training), value = TRUE)
predictors_IL <- predictors[, IL_str]
predictors_IL <- predictors[, IL_str]
df <- data.frame(diagnosis, predictors_IL)
inTrain = createDataPartition(df$diagnosis, p = 3/4)[[1]]
training = df[inTrain, ]
testing = df[-inTrain, ]
modelFit <- train(diagnosis ~ ., method = "glm", data = training)
modelFit <- train(diagnosis ~ ., method = "glm", data = training)
colnames(df)
install.packages("e1071")
modelFit <- train(diagnosis ~ ., method = "glm", data = training)
predictions <- predict(modelFit, newdata = testing)
C1 <- confusionMatrix(predictions, testing$diagnosis)
print(C1)
modelFit <- train(training$diagnosis ~ ., method = "glm", data = training,
preProcess = "pca",
Control = trainControl(preProcOptions = list(thresh = 0.8)))
preObj <- preProcess(training, method ="pca")
setwd("C:/Users/mikel/developing-data-products/slidify")
